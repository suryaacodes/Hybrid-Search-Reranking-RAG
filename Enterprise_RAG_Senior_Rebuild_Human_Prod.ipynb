{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c2f576",
   "metadata": {},
   "source": [
    "# Enterprise RAG (production-minded rebuild)\n",
    "\n",
    "**Goal:** a RAG pipeline you can explain, debug, and extend — not a demo that “looks cool”.\n",
    "\n",
    "**Constraints:**\n",
    "- No private company docs (everything is synthetic but shaped like real policies/runbooks)\n",
    "- Keep it runnable on a laptop / Colab\n",
    "- Make retrieval decisions visible (scores, sources, eval)\n",
    "\n",
    "**What’s implemented:**\n",
    "- ~65 realistic docs → chunking → hybrid retrieval (BM25 + dense) → reranking\n",
    "- Prompt payload with citations (LLM-agnostic)\n",
    "- Small eval set + retrieval metrics\n",
    "- A **real FastAPI service** under `app/` (`/ingest`, `/search`, `/ask`)\n",
    "\n",
    "**What’s next (if you want to go “work-grade”):**\n",
    "- persistence (Chroma/pgvector), auth, rate limits\n",
    "- tracing (OpenTelemetry), offline eval runs in CI\n",
    "- background ingestion + incremental reindexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe1be8",
   "metadata": {},
   "source": [
    "## 0) Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U \"sentence-transformers>=3.0.0\" faiss-cpu rank-bm25 rapidfuzz pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random, hashlib, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d714af",
   "metadata": {},
   "source": [
    "## 1) Configuration + utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RagConfig:\n",
    "    seed: int = 42\n",
    "    chunk_size: int = 650\n",
    "    chunk_overlap: int = 120\n",
    "    dense_k: int = 20\n",
    "    bm25_k: int = 20\n",
    "    final_k: int = 5\n",
    "    embed_model: str = \"intfloat/e5-small-v2\"\n",
    "    rerank_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    normalize_embeddings: bool = True\n",
    "\n",
    "CFG = RagConfig()\n",
    "random.seed(CFG.seed)\n",
    "np.random.seed(CFG.seed)\n",
    "\n",
    "def simple_hash(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    return \" \".join(s.strip().split())\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    text = clean_text(text)\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "def bm25_tokenize(s: str) -> List[str]:\n",
    "    s = s.lower()\n",
    "    return [t for t in re.split(r\"[^a-z0-9]+\", s) if t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230137ee",
   "metadata": {},
   "source": [
    "## 2) Create realistic sample corpus (65 docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample_corpus(n: int = 65) -> List[Dict[str, Any]]:\n",
    "    base_docs = [\n",
    "        (\"Expense Reimbursement Policy\", \"Finance\",\n",
    "         \"Employees must submit receipts within 30 days of the expense date via the Finance Portal. \"\n",
    "         \"Meals are reimbursed up to $65/day domestically. Alcohol is not reimbursable. \"\n",
    "         \"Managers approve expenses within 5 business days. Late submissions require VP approval.\"),\n",
    "        (\"Travel Booking Guidelines\", \"Finance\",\n",
    "         \"Air travel must be booked through the approved travel vendor. Economy class for flights under 6 hours. \"\n",
    "         \"For flights over 6 hours, premium economy is allowed with manager approval. \"\n",
    "         \"Use the corporate card when possible.\"),\n",
    "        (\"Paid Time Off (PTO) Overview\", \"HR\",\n",
    "         \"Full-time employees accrue PTO each pay period. PTO requests should be submitted at least 10 days in advance. \"\n",
    "         \"Carryover is capped at 40 hours. Unused PTO above the cap is forfeited on January 31.\"),\n",
    "        (\"Parental Leave Policy\", \"HR\",\n",
    "         \"Eligible employees receive 12 weeks of paid parental leave. Leave can be taken within 12 months of birth or adoption. \"\n",
    "         \"Benefits continue during leave. Coordinate return-to-work plans with HR.\"),\n",
    "        (\"Information Security Password Standard\", \"Security\",\n",
    "         \"Passwords must be at least 14 characters and include a mix of letters and numbers. \"\n",
    "         \"MFA is required for all remote access. Passwords may not be reused for 12 cycles. \"\n",
    "         \"Sharing passwords is prohibited.\"),\n",
    "        (\"Incident Response: Severity Levels\", \"Security\",\n",
    "         \"SEV-1: full outage or major security incident; page on-call immediately. \"\n",
    "         \"SEV-2: partial degradation; engage incident commander within 30 minutes. \"\n",
    "         \"SEV-3: limited impact; handle in business hours unless escalated.\"),\n",
    "        (\"On-Call Runbook: Service Restart\", \"IT\",\n",
    "         \"Before restarting a service, verify downstream dependencies are healthy. \"\n",
    "         \"Use the deployment dashboard to initiate a rolling restart. \"\n",
    "         \"If error rate spikes, rollback to the last known good version.\"),\n",
    "        (\"VPN Access Procedure\", \"IT\",\n",
    "         \"VPN access is granted after completing security training. Requests are submitted in the Access Portal. \"\n",
    "         \"Access is reviewed quarterly. Lost devices must be reported within 1 hour.\"),\n",
    "        (\"Code Review Guidelines\", \"Engineering\",\n",
    "         \"All production code requires at least one approving review. \"\n",
    "         \"High-risk changes (auth, payments) require two approvals. \"\n",
    "         \"Reviewers must check tests, security implications, and operational impact.\"),\n",
    "        (\"Release Process (Weekly)\", \"Engineering\",\n",
    "         \"We ship weekly on Wednesdays. Feature flags are required for user-facing changes. \"\n",
    "         \"A release candidate is cut Tuesday 3 PM ET. Rollback plans must be documented.\"),\n",
    "        (\"Data Retention Policy\", \"Legal\",\n",
    "         \"Customer support tickets are retained for 2 years. Audit logs are retained for 1 year. \"\n",
    "         \"PII must be deleted upon verified request within 30 days, unless retention is legally required.\"),\n",
    "        (\"Customer Refund Policy\", \"Support\",\n",
    "         \"Refunds are available within 14 days of purchase if usage is under 10%. \"\n",
    "         \"Refunds outside the window require escalation. Refund processing time is 5-7 business days.\"),\n",
    "        (\"SLA: API Availability\", \"Engineering\",\n",
    "         \"API uptime target is 99.9% monthly. Maintenance windows are announced 72 hours in advance. \"\n",
    "         \"SLA credits apply if downtime exceeds 45 minutes in a month.\"),\n",
    "        (\"Laptop Asset Policy\", \"IT\",\n",
    "         \"Corporate laptops must use full-disk encryption. Devices are replaced every 4 years. \"\n",
    "         \"Personal software installations require approval. Report theft within 24 hours.\"),\n",
    "    ]\n",
    "\n",
    "    products = [\"Atlas\", \"Beacon\", \"Cobalt\", \"Delta\", \"Echo\", \"Fjord\", \"Glimmer\", \"Helios\"]\n",
    "    systems = [\"Payments\", \"Identity\", \"Search\", \"Messaging\", \"Analytics\", \"Billing\", \"Notifications\"]\n",
    "    regions = [\"US\", \"EU\", \"APAC\"]\n",
    "\n",
    "    templated = []\n",
    "    for p in products:\n",
    "        templated.append((f\"{p} Service Overview\", \"Engineering\",\n",
    "                          f\"{p} is a core platform service used by internal teams. \"\n",
    "                          f\"It exposes REST endpoints and publishes events to the message bus. \"\n",
    "                          f\"Primary SLOs: latency p95 < 250ms and error rate < 1%.\"))\n",
    "        templated.append((f\"{p} Access Control\", \"Security\",\n",
    "                          f\"Access to {p} is granted via role-based groups. \"\n",
    "                          f\"Privileged actions require just-in-time access for 2 hours. \"\n",
    "                          f\"All access is logged and reviewed monthly.\"))\n",
    "    for s in systems:\n",
    "        templated.append((f\"Runbook: {s} High Latency\", \"IT\",\n",
    "                          f\"If {s} p95 latency exceeds threshold for 10 minutes, page on-call. \"\n",
    "                          f\"Check recent deploys, database saturation, and cache hit rate. \"\n",
    "                          f\"Mitigation: scale replicas, enable rate limiting, or rollback.\"))\n",
    "    for r in regions:\n",
    "        templated.append((f\"Data Residency - {r}\", \"Legal\",\n",
    "                          f\"Customer data stored in {r} must remain within {r} boundaries. \"\n",
    "                          f\"Cross-region replication requires legal approval and documented justification.\"))\n",
    "\n",
    "    all_docs = base_docs + templated\n",
    "    random.shuffle(all_docs)\n",
    "\n",
    "    docs = []\n",
    "    for i, (title, dept, text) in enumerate(all_docs[:n], start=1):\n",
    "        doc_id = f\"DOC-{i:03d}-{simple_hash(title)}\"\n",
    "        docs.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"department\": dept,\n",
    "            \"text\": clean_text(text)\n",
    "        })\n",
    "    return docs\n",
    "\n",
    "corpus = build_sample_corpus(n=65)\n",
    "print(\"Docs:\", len(corpus))\n",
    "for d in corpus[:5]:\n",
    "    print(d[\"doc_id\"], \"|\", d[\"department\"], \"|\", d[\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2c108",
   "metadata": {},
   "source": [
    "## 3) Chunk + index (BM25 + FAISS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782af6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chunks(corpus: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    for doc in corpus:\n",
    "        parts = chunk_text(doc[\"text\"], CFG.chunk_size, CFG.chunk_overlap)\n",
    "        for j, part in enumerate(parts):\n",
    "            chunks.append({\n",
    "                \"chunk_id\": f'{doc[\"doc_id\"]}::C{j:02d}',\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"department\": doc[\"department\"],\n",
    "                \"text\": part\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "chunks = build_chunks(corpus)\n",
    "print(f\"Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_tokens = [bm25_tokenize(c[\"text\"]) for c in chunks]\n",
    "bm25 = BM25Okapi(bm25_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(CFG.embed_model)\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    emb = embedder.encode(texts, batch_size=64, normalize_embeddings=CFG.normalize_embeddings, show_progress_bar=True)\n",
    "    return np.asarray(emb, dtype=\"float32\")\n",
    "\n",
    "chunk_emb = embed_texts([c[\"text\"] for c in chunks])\n",
    "dim = chunk_emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(chunk_emb)\n",
    "print(\"FAISS size:\", index.ntotal, \"dim:\", dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4552ab2",
   "metadata": {},
   "source": [
    "## 4) Hybrid retrieval + reranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f43bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(CFG.rerank_model)\n",
    "\n",
    "def dense_search(query: str, k: int) -> List[Tuple[int, float]]:\n",
    "    q = embed_texts([query])\n",
    "    scores, ids = index.search(q, k)\n",
    "    return [(int(i), float(s)) for i, s in zip(ids[0], scores[0]) if i != -1]\n",
    "\n",
    "def bm25_search(query: str, k: int) -> List[Tuple[int, float]]:\n",
    "    scores = bm25.get_scores(bm25_tokenize(query))\n",
    "    top = np.argsort(scores)[::-1][:k]\n",
    "    return [(int(i), float(scores[i])) for i in top]\n",
    "\n",
    "def hybrid_candidates(query: str) -> Dict[int, Dict[str, float]]:\n",
    "    cand: Dict[int, Dict[str, float]] = {}\n",
    "    for idx, s in dense_search(query, CFG.dense_k):\n",
    "        cand.setdefault(idx, {})[\"dense_score\"] = s\n",
    "    for idx, s in bm25_search(query, CFG.bm25_k):\n",
    "        cand.setdefault(idx, {})[\"bm25_score\"] = s\n",
    "    return cand\n",
    "\n",
    "def rerank(query: str, candidate_idxs: List[int]) -> List[Tuple[int, float]]:\n",
    "    pairs = [(query, chunks[i][\"text\"]) for i in candidate_idxs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(candidate_idxs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [(int(i), float(s)) for i, s in ranked]\n",
    "\n",
    "def retrieve(query: str, k: int = None) -> List[Dict[str, Any]]:\n",
    "    k = k or CFG.final_k\n",
    "    cand = hybrid_candidates(query)\n",
    "    candidate_idxs = list(cand.keys())\n",
    "    ranked = rerank(query, candidate_idxs)[:k]\n",
    "\n",
    "    out = []\n",
    "    for idx, rr_score in ranked:\n",
    "        meta = cand.get(idx, {})\n",
    "        c = chunks[idx]\n",
    "        out.append({\n",
    "            \"chunk_id\": c[\"chunk_id\"],\n",
    "            \"doc_id\": c[\"doc_id\"],\n",
    "            \"title\": c[\"title\"],\n",
    "            \"department\": c[\"department\"],\n",
    "            \"text\": c[\"text\"],\n",
    "            \"rerank_score\": float(rr_score),\n",
    "            \"dense_score\": meta.get(\"dense_score\"),\n",
    "            \"bm25_score\": meta.get(\"bm25_score\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def pretty_results(results: List[Dict[str, Any]]):\n",
    "    for i, r in enumerate(results, start=1):\n",
    "        print(f\"\\n=== #{i} | {r['title']} ({r['doc_id']}) [{r['department']}] ===\")\n",
    "        print(f\"rerank={r['rerank_score']:.4f} | dense={r['dense_score']} | bm25={r['bm25_score']}\")\n",
    "        print(r[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d0c60",
   "metadata": {},
   "source": [
    "## 5) Demo retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725afd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_queries = [\n",
    "    \"What is the refund window for a customer purchase?\",\n",
    "    \"How many days do I have to submit an expense reimbursement receipt?\",\n",
    "    \"When should SEV-1 incidents page on-call?\",\n",
    "    \"Do we require MFA for remote access?\",\n",
    "    \"What is the weekly release day and when do we cut the release candidate?\"\n",
    "]\n",
    "for q in demo_queries:\n",
    "    print(\"\\n\\n#\", q)\n",
    "    pretty_results(retrieve(q))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec7c902",
   "metadata": {},
   "source": [
    "## 6) LLM-ready prompt payload (citations included)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_payload(question: str, contexts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    sources = [{\"doc_id\": c[\"doc_id\"], \"title\": c[\"title\"], \"chunk_id\": c[\"chunk_id\"]} for c in contexts]\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        [f\"[{i+1}] {c['title']} ({c['doc_id']}):\\n{c['text']}\" for i, c in enumerate(contexts)]\n",
    "    )\n",
    "\n",
    "    system = (\n",
    "        \"You are an enterprise assistant. Answer ONLY from the provided context. \"\n",
    "        \"If the answer is not in the context, say you don't know and ask one clarifying question.\"\n",
    "    )\n",
    "    user = f\"Question: {question}\\n\\nContext:\\n{context_text}\\n\\nAnswer with citations like [1], [2].\"\n",
    "    return {\"system\": system, \"user\": user, \"sources\": sources}\n",
    "\n",
    "payload = build_prompt_payload(\"What is the refund window?\", retrieve(\"refund window\"))\n",
    "print(payload[\"user\"][:500])\n",
    "print(\"Sources:\", payload[\"sources\"][:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a097deb",
   "metadata": {},
   "source": [
    "## 7) Evaluation (35 Q/A) + metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_set(corpus: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    title_to_id = {d[\"title\"]: d[\"doc_id\"] for d in corpus}\n",
    "\n",
    "    seeds = [\n",
    "        (\"Customer Refund Policy\", \"refund window\"),\n",
    "        (\"Expense Reimbursement Policy\", \"receipts within 30 days\"),\n",
    "        (\"Information Security Password Standard\", \"MFA required\"),\n",
    "        (\"Incident Response: Severity Levels\", \"SEV-1 page on-call\"),\n",
    "        (\"Paid Time Off (PTO) Overview\", \"PTO carryover cap\"),\n",
    "        (\"Release Process (Weekly)\", \"weekly release day\"),\n",
    "        (\"Code Review Guidelines\", \"how many approvals\"),\n",
    "        (\"Data Retention Policy\", \"support ticket retention\"),\n",
    "        (\"VPN Access Procedure\", \"VPN access reviewed\"),\n",
    "        (\"Laptop Asset Policy\", \"laptop replacement cycle\"),\n",
    "        (\"SLA: API Availability\", \"uptime target\"),\n",
    "        (\"Travel Booking Guidelines\", \"economy class 6 hours\"),\n",
    "        (\"Parental Leave Policy\", \"weeks of paid parental leave\"),\n",
    "    ]\n",
    "\n",
    "    eval_items = []\n",
    "    for title, q in seeds:\n",
    "        if title in title_to_id:\n",
    "            eval_items.append({\n",
    "                \"question\": q if q.endswith(\"?\") else q + \"?\",\n",
    "                \"expected_doc_id\": title_to_id[title],\n",
    "                \"expected_title\": title,\n",
    "            })\n",
    "\n",
    "    # inferred templates\n",
    "    products, systems, regions = [], [], []\n",
    "    for d in corpus:\n",
    "        if d[\"title\"].endswith(\" Service Overview\"):\n",
    "            products.append(d[\"title\"].replace(\" Service Overview\", \"\"))\n",
    "        if d[\"title\"].startswith(\"Runbook: \") and d[\"title\"].endswith(\" High Latency\"):\n",
    "            systems.append(d[\"title\"].replace(\"Runbook: \", \"\").replace(\" High Latency\", \"\"))\n",
    "        if d[\"title\"].startswith(\"Data Residency - \"):\n",
    "            regions.append(d[\"title\"].replace(\"Data Residency - \", \"\"))\n",
    "\n",
    "    for p in products[:8]:\n",
    "        t = f\"{p} Access Control\"\n",
    "        if t in title_to_id:\n",
    "            eval_items.append({\n",
    "                \"question\": f\"How is access granted to {p}?\",\n",
    "                \"expected_doc_id\": title_to_id[t],\n",
    "                \"expected_title\": t,\n",
    "            })\n",
    "    for s in systems[:8]:\n",
    "        t = f\"Runbook: {s} High Latency\"\n",
    "        if t in title_to_id:\n",
    "            eval_items.append({\n",
    "                \"question\": f\"What should I check when {s} latency is high?\",\n",
    "                \"expected_doc_id\": title_to_id[t],\n",
    "                \"expected_title\": t,\n",
    "            })\n",
    "    for r in regions[:3]:\n",
    "        t = f\"Data Residency - {r}\"\n",
    "        if t in title_to_id:\n",
    "            eval_items.append({\n",
    "                \"question\": f\"Can customer data be replicated out of {r}?\",\n",
    "                \"expected_doc_id\": title_to_id[t],\n",
    "                \"expected_title\": t,\n",
    "            })\n",
    "\n",
    "    # Paraphrases to reach ~35\n",
    "    out = []\n",
    "    for item in eval_items:\n",
    "        out.append(item)\n",
    "        out.append({**item, \"question\": \"Please summarize: \" + item[\"question\"]})\n",
    "        out.append({**item, \"question\": \"What does our policy say about \" + item[\"question\"].rstrip(\"?\").lower() + \"?\"})\n",
    "\n",
    "    out = out[:35]\n",
    "    random.shuffle(out)\n",
    "    return out\n",
    "\n",
    "eval_set = build_eval_set(corpus)\n",
    "print(\"Eval examples:\", len(eval_set))\n",
    "print(eval_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved_doc_ids: List[str], expected: str, k: int) -> float:\n",
    "    return 1.0 if expected in retrieved_doc_ids[:k] else 0.0\n",
    "\n",
    "def reciprocal_rank(retrieved_doc_ids: List[str], expected: str) -> float:\n",
    "    for i, doc_id in enumerate(retrieved_doc_ids, start=1):\n",
    "        if doc_id == expected:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(retrieved_doc_ids: List[str], expected: str, k: int) -> float:\n",
    "    dcg = 0.0\n",
    "    for i, doc_id in enumerate(retrieved_doc_ids[:k], start=1):\n",
    "        if doc_id == expected:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    return dcg  # IDCG=1.0 for binary relevance at rank 1\n",
    "\n",
    "def evaluate(eval_set: List[Dict[str, Any]], k: int = 5) -> Dict[str, Any]:\n",
    "    pks, mrrs, ndcgs = [], [], []\n",
    "    failures = []\n",
    "    for ex in eval_set:\n",
    "        results = retrieve(ex[\"question\"], k=k)\n",
    "        doc_ids = [r[\"doc_id\"] for r in results]\n",
    "        pks.append(precision_at_k(doc_ids, ex[\"expected_doc_id\"], k))\n",
    "        mrrs.append(reciprocal_rank(doc_ids, ex[\"expected_doc_id\"]))\n",
    "        ndcgs.append(ndcg_at_k(doc_ids, ex[\"expected_doc_id\"], k))\n",
    "        if ex[\"expected_doc_id\"] not in doc_ids:\n",
    "            failures.append({\n",
    "                \"question\": ex[\"question\"],\n",
    "                \"expected\": ex[\"expected_title\"],\n",
    "                \"got\": [r[\"title\"] for r in results]\n",
    "            })\n",
    "    return {\n",
    "        f\"Precision@{k}\": float(np.mean(pks)),\n",
    "        \"MRR\": float(np.mean(mrrs)),\n",
    "        f\"nDCG@{k}\": float(np.mean(ndcgs)),\n",
    "        \"failures\": failures\n",
    "    }\n",
    "\n",
    "metrics = evaluate(eval_set, k=5)\n",
    "print({k:v for k,v in metrics.items() if k != \"failures\"})\n",
    "print(\"Failures:\", len(metrics[\"failures\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660302f",
   "metadata": {},
   "source": [
    "## 8) Save corpus + eval to `data/` (GitHub-friendly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out_dir = Path(\"data\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(out_dir/\"sample_corpus.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in corpus:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(out_dir/\"eval_set.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_set, f, indent=2)\n",
    "\n",
    "print(\"Wrote:\", out_dir/\"sample_corpus.jsonl\")\n",
    "print(\"Wrote:\", out_dir/\"eval_set.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d5b3e",
   "metadata": {},
   "source": [
    "## 9) FastAPI service (real files under `app/`)\n",
    "\n",
    "This writes a runnable service to disk (no more blueprint string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a real FastAPI app/ layout\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"app\")\n",
    "(root / \"routers\").mkdir(parents=True, exist_ok=True)\n",
    "(root / \"core\").mkdir(parents=True, exist_ok=True)\n",
    "(root / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(root / \"routers\" / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "(root / \"core\" / \"__init__.py\").write_text(\"\", encoding=\"utf-8\")\n",
    "print(\"Created:\", root.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "fastapi==0.111.0\n",
    "uvicorn[standard]==0.30.1\n",
    "pydantic==2.7.4\n",
    "python-dotenv==1.0.1\n",
    "\n",
    "# Retrieval stack\n",
    "rank-bm25==0.2.2\n",
    "rapidfuzz==3.9.6\n",
    "\n",
    "# Dense + reranker (pins to avoid numpy/scipy breakage)\n",
    "numpy==1.26.4\n",
    "scipy==1.11.4\n",
    "sentence-transformers==3.0.1\n",
    "faiss-cpu==1.8.0.post1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e940af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/core/config.py\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Settings(BaseModel):\n",
    "    # Models\n",
    "    embed_model: str = Field(default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    rerank_model: str = Field(default=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "    # Retrieval knobs (tune these, don’t hardcode in random places)\n",
    "    chunk_size: int = 650\n",
    "    chunk_overlap: int = 120\n",
    "    bm25_weight: float = 0.35\n",
    "    dense_k: int = 25\n",
    "    bm25_k: int = 40\n",
    "    rerank_k: int = 8\n",
    "\n",
    "    # Storage (simple local persistence for portfolio)\n",
    "    data_dir: str = \"data\"\n",
    "    index_dir: str = \"data/index\"\n",
    "\n",
    "settings = Settings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/schemas.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class DocIn(BaseModel):\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    domain: str\n",
    "    text: str\n",
    "    version: Optional[str] = None\n",
    "\n",
    "class IngestRequest(BaseModel):\n",
    "    docs: List[DocIn] = Field(..., min_length=1)\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    k: int = 8\n",
    "\n",
    "class AskRequest(BaseModel):\n",
    "    question: str\n",
    "    k: int = 8\n",
    "\n",
    "class ContextHit(BaseModel):\n",
    "    doc_id: str\n",
    "    chunk_id: str\n",
    "    title: str\n",
    "    domain: str\n",
    "    text: str\n",
    "    score: float\n",
    "    signals: Dict[str, Any] = {}\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    query: str\n",
    "    hits: List[ContextHit]\n",
    "\n",
    "class AskResponse(BaseModel):\n",
    "    question: str\n",
    "    hits: List[ContextHit]\n",
    "    prompt: Dict[str, Any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/core/text_utils.py\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "_word = re.compile(r\"[A-Za-z0-9_./-]+\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # Keeps tokens like \"S3\", \"PCI-DSS\", \"v1.2\", \"PAY-1234\"\n",
    "    return [t.lower() for t in _word.findall(text)]\n",
    "\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return \" \".join(s.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e600331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/core/chunking.py\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def chunk_doc(doc: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:\n",
    "    text = doc[\"text\"]\n",
    "    # Simple token-ish chunking (works fine for portfolio); swap for semantic chunking later.\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    idx = 0\n",
    "    while start < len(words):\n",
    "        end = min(len(words), start + chunk_size)\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words).strip()\n",
    "        chunks.append({\n",
    "            \"chunk_id\": f\"{doc['doc_id']}::c{idx}\",\n",
    "            \"doc_id\": doc[\"doc_id\"],\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"domain\": doc.get(\"domain\", \"Unknown\"),\n",
    "            \"text\": chunk_text,\n",
    "            \"start_word\": start,\n",
    "            \"end_word\": end,\n",
    "        })\n",
    "        idx += 1\n",
    "        if end == len(words):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/core/index.py\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from .text_utils import tokenize, normalize_ws\n",
    "from .chunking import chunk_doc\n",
    "\n",
    "# Optional heavy deps (we fail loudly with clear message)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "except Exception as e:  # pragma: no cover\n",
    "    np = None\n",
    "    faiss = None\n",
    "    SentenceTransformer = None\n",
    "    CrossEncoder = None\n",
    "    _IMPORT_ERR = e\n",
    "else:\n",
    "    _IMPORT_ERR = None\n",
    "\n",
    "@dataclass\n",
    "class BuiltIndex:\n",
    "    chunks: List[Dict[str, Any]]\n",
    "    bm25: BM25Okapi\n",
    "    dense_index: Any  # faiss index\n",
    "    embeddings: Any   # np.ndarray\n",
    "    embedder: Any\n",
    "    reranker: Any\n",
    "\n",
    "class RagIndex:\n",
    "    def __init__(self, settings):\n",
    "        self.s = settings\n",
    "        self._built: BuiltIndex | None = None\n",
    "\n",
    "    # ------------ Build / Persist ------------\n",
    "    def build(self, docs: List[Dict[str, Any]]) -> None:\n",
    "        if _IMPORT_ERR is not None:\n",
    "            raise RuntimeError(\n",
    "                \"Dense/reranker deps failed to import. \"\n",
    "                \"Install pinned requirements and restart. Original error: \"\n",
    "                f\"{_IMPORT_ERR}\"\n",
    "            )\n",
    "\n",
    "        # Chunk\n",
    "        chunks: List[Dict[str, Any]] = []\n",
    "        for d in docs:\n",
    "            d = {**d, \"text\": normalize_ws(d[\"text\"])}\n",
    "            chunks.extend(chunk_doc(d, self.s.chunk_size, self.s.chunk_overlap))\n",
    "\n",
    "        # BM25\n",
    "        tokenized = [tokenize(c[\"text\"]) for c in chunks]\n",
    "        bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "        # Dense index (FAISS)\n",
    "        embedder = SentenceTransformer(self.s.embed_model)\n",
    "        chunk_texts = [c[\"text\"] for c in chunks]\n",
    "        embs = embedder.encode(chunk_texts, normalize_embeddings=True, batch_size=64, show_progress_bar=False)\n",
    "        embs = np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "        dim = embs.shape[1]\n",
    "        dense_index = faiss.IndexFlatIP(dim)\n",
    "        dense_index.add(embs)\n",
    "\n",
    "        # Reranker\n",
    "        reranker = CrossEncoder(self.s.rerank_model)\n",
    "\n",
    "        self._built = BuiltIndex(\n",
    "            chunks=chunks,\n",
    "            bm25=bm25,\n",
    "            dense_index=dense_index,\n",
    "            embeddings=embs,\n",
    "            embedder=embedder,\n",
    "            reranker=reranker,\n",
    "        )\n",
    "\n",
    "    def save(self, index_dir: str) -> None:\n",
    "        assert self._built is not None, \"Index not built\"\n",
    "        p = Path(index_dir)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Chunks\n",
    "        (p / \"chunks.jsonl\").write_text(\n",
    "            \"\\n\".join(json.dumps(c, ensure_ascii=False) for c in self._built.chunks),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "        # FAISS index + embeddings\n",
    "        faiss.write_index(self._built.dense_index, str(p / \"faiss.index\"))\n",
    "        np.save(p / \"embeddings.npy\", self._built.embeddings)\n",
    "\n",
    "        # BM25 needs tokenized corpus; we rebuild from chunks on load\n",
    "        (p / \"meta.json\").write_text(json.dumps({\n",
    "            \"embed_model\": self.s.embed_model,\n",
    "            \"rerank_model\": self.s.rerank_model,\n",
    "            \"chunk_size\": self.s.chunk_size,\n",
    "            \"chunk_overlap\": self.s.chunk_overlap,\n",
    "            \"bm25_weight\": self.s.bm25_weight,\n",
    "        }, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    def load(self, index_dir: str) -> None:\n",
    "        if _IMPORT_ERR is not None:\n",
    "            raise RuntimeError(\n",
    "                \"Dense/reranker deps failed to import. \"\n",
    "                \"Install pinned requirements and restart. Original error: \"\n",
    "                f\"{_IMPORT_ERR}\"\n",
    "            )\n",
    "        p = Path(index_dir)\n",
    "        chunks = [json.loads(line) for line in (p / \"chunks.jsonl\").read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "        tokenized = [tokenize(c[\"text\"]) for c in chunks]\n",
    "        bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "        embedder = SentenceTransformer(self.s.embed_model)\n",
    "        reranker = CrossEncoder(self.s.rerank_model)\n",
    "\n",
    "        dense_index = faiss.read_index(str(p / \"faiss.index\"))\n",
    "        embs = np.load(p / \"embeddings.npy\")\n",
    "\n",
    "        self._built = BuiltIndex(\n",
    "            chunks=chunks,\n",
    "            bm25=bm25,\n",
    "            dense_index=dense_index,\n",
    "            embeddings=embs,\n",
    "            embedder=embedder,\n",
    "            reranker=reranker,\n",
    "        )\n",
    "\n",
    "    # ------------ Retrieval ------------\n",
    "    def search(self, query: str, k: int) -> List[Dict[str, Any]]:\n",
    "        assert self._built is not None, \"Index not ready\"\n",
    "\n",
    "        # BM25 scores\n",
    "        q_tokens = tokenize(query)\n",
    "        bm25_scores = self._built.bm25.get_scores(q_tokens)\n",
    "\n",
    "        # Dense scores\n",
    "        q_emb = self._built.embedder.encode([query], normalize_embeddings=True, show_progress_bar=False)\n",
    "        q_emb = np.asarray(q_emb, dtype=\"float32\")\n",
    "        dense_scores, dense_ids = self._built.dense_index.search(q_emb, self.s.dense_k)\n",
    "\n",
    "        dense_map = {int(i): float(s) for i, s in zip(dense_ids[0], dense_scores[0]) if i != -1}\n",
    "\n",
    "        # Hybrid combine (simple weighted sum after min-max normalization)\n",
    "        bm25 = np.asarray(bm25_scores, dtype=\"float32\")\n",
    "        if bm25.max() > bm25.min():\n",
    "            bm25_n = (bm25 - bm25.min()) / (bm25.max() - bm25.min())\n",
    "        else:\n",
    "            bm25_n = bm25 * 0.0\n",
    "\n",
    "        dense = np.zeros_like(bm25_n)\n",
    "        for i, s in dense_map.items():\n",
    "            dense[i] = s\n",
    "        if dense.max() > dense.min():\n",
    "            dense_n = (dense - dense.min()) / (dense.max() - dense.min())\n",
    "        else:\n",
    "            dense_n = dense * 0.0\n",
    "\n",
    "        w = self.s.bm25_weight\n",
    "        hybrid = (w * bm25_n) + ((1 - w) * dense_n)\n",
    "\n",
    "        # Candidate pool\n",
    "        cand_ids = np.argsort(-hybrid)[: max(self.s.bm25_k, self.s.dense_k)].tolist()\n",
    "        cands = [self._built.chunks[i] for i in cand_ids]\n",
    "\n",
    "        # Rerank top candidates\n",
    "        pairs = [(query, c[\"text\"]) for c in cands]\n",
    "        rr_scores = self._built.reranker.predict(pairs)\n",
    "        rr_scores = [float(x) for x in rr_scores]\n",
    "\n",
    "        reranked = sorted(zip(cands, rr_scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        hits = []\n",
    "        for c, s in reranked:\n",
    "            hits.append({\n",
    "                **c,\n",
    "                \"score\": s,\n",
    "                \"signals\": {\n",
    "                    \"bm25_weight\": self.s.bm25_weight,\n",
    "                    \"candidate_pool\": len(cands),\n",
    "                }\n",
    "            })\n",
    "        return hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d51187",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/core/prompting.py\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def build_prompt_payload(question: str, hits: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    citations = [{\"doc_id\": h[\"doc_id\"], \"chunk_id\": h[\"chunk_id\"], \"title\": h[\"title\"]} for h in hits]\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        [f\"[{i+1}] {h['title']} ({h['domain']})\\n{h['text']}\" for i, h in enumerate(hits)]\n",
    "    )\n",
    "\n",
    "    system = (\n",
    "        \"You are an internal assistant. Answer using ONLY the provided context. \"\n",
    "        \"If the answer isn't in context, say you don't know and ask for the missing policy/runbook.\"\n",
    "    )\n",
    "    user = f\"Question: {question}\\n\\nContext:\\n{context_text}\\n\\nReturn a concise answer + cite sources like [1], [2].\"\n",
    "\n",
    "    return {\"system\": system, \"user\": user, \"citations\": citations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/routers/ingest.py\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from typing import Dict, Any\n",
    "\n",
    "from ..schemas import IngestRequest\n",
    "from ..services import rag_service\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.post(\"/ingest\")\n",
    "def ingest(req: IngestRequest) -> Dict[str, Any]:\n",
    "    # Simple: rebuild whole index. (Production: background job + incremental updates.)\n",
    "    docs = [d.model_dump() for d in req.docs]\n",
    "    try:\n",
    "        rag_service.build_and_persist(docs)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "    return {\"ok\": True, \"docs\": len(docs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/routers/search.py\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from typing import Dict, Any\n",
    "\n",
    "from ..schemas import SearchRequest, SearchResponse, ContextHit\n",
    "from ..services import rag_service\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.post(\"/search\", response_model=SearchResponse)\n",
    "def search(req: SearchRequest):\n",
    "    try:\n",
    "        hits = rag_service.search(req.query, k=req.k)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    return {\n",
    "        \"query\": req.query,\n",
    "        \"hits\": [ContextHit(**h) for h in hits]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fe54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/routers/ask.py\n",
    "from fastapi import APIRouter, HTTPException\n",
    "\n",
    "from ..schemas import AskRequest, AskResponse, ContextHit\n",
    "from ..services import rag_service\n",
    "\n",
    "router = APIRouter()\n",
    "\n",
    "@router.post(\"/ask\", response_model=AskResponse)\n",
    "def ask(req: AskRequest):\n",
    "    try:\n",
    "        hits = rag_service.search(req.question, k=req.k)\n",
    "        prompt = rag_service.build_prompt(req.question, hits)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    return {\n",
    "        \"question\": req.question,\n",
    "        \"hits\": [ContextHit(**h) for h in hits],\n",
    "        \"prompt\": prompt,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c59ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/services.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from .core.config import settings\n",
    "from .core.index import RagIndex\n",
    "from .core.prompting import build_prompt_payload\n",
    "\n",
    "class RagService:\n",
    "    def __init__(self):\n",
    "        self.index = RagIndex(settings)\n",
    "        self.index_dir = settings.index_dir\n",
    "\n",
    "    def ensure_loaded(self) -> None:\n",
    "        p = Path(self.index_dir)\n",
    "        if (p / \"chunks.jsonl\").exists():\n",
    "            self.index.load(self.index_dir)\n",
    "\n",
    "    def build_and_persist(self, docs: List[Dict[str, Any]]) -> None:\n",
    "        self.index.build(docs)\n",
    "        self.index.save(self.index_dir)\n",
    "\n",
    "    def search(self, query: str, k: int) -> List[Dict[str, Any]]:\n",
    "        return self.index.search(query, k=k)\n",
    "\n",
    "    def build_prompt(self, question: str, hits: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        return build_prompt_payload(question, hits)\n",
    "\n",
    "rag_service = RagService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538dbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/main.py\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from .routers import ingest, search, ask\n",
    "from .services import rag_service\n",
    "\n",
    "app = FastAPI(title=\"Enterprise RAG\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def _startup():\n",
    "    # Load persisted index if present.\n",
    "    rag_service.ensure_loaded()\n",
    "\n",
    "app.include_router(ingest.router, tags=[\"ingest\"])\n",
    "app.include_router(search.router, tags=[\"search\"])\n",
    "app.include_router(ask.router, tags=[\"ask\"])\n",
    "\n",
    "@app.get(\"/healthz\")\n",
    "def healthz():\n",
    "    return {\"ok\": True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check: show the generated tree\n",
    "from pathlib import Path\n",
    "for p in sorted(Path(\"app\").rglob(\"*\")):\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd7b1a",
   "metadata": {},
   "source": [
    "## 10) Engineering decisions + failure cases (what makes this feel real)\n",
    "\n",
    "### Decisions (and why)\n",
    "\n",
    "- **Chunk size (~650 words) + overlap (120 words)**  \n",
    "  Big enough to keep policy clauses together (eligibility + exceptions), small enough that reranker can focus.  \n",
    "  If chunks are too small → you lose the “exception” sentence. Too big → unrelated clauses dilute scoring.\n",
    "\n",
    "- **Hybrid weighting (BM25 weight ~0.35)**  \n",
    "  Dense retrieval is great for paraphrases; BM25 rescues **IDs, numbers, acronyms** (“PCI”, “30 days”, “S3”, ticket codes).  \n",
    "  0.35 is a compromise: enough lexical “snap” without letting keyword spam dominate.\n",
    "\n",
    "- **Reranker choice: `ms-marco-MiniLM-L-6-v2` (portfolio-friendly)**  \n",
    "  Pros: fast, cheap, good enough to show the “precision step”.  \n",
    "  Tradeoff: smaller cross-encoders can miss subtle negations / long-distance constraints. In production you’d evaluate bigger rerankers vs latency budgets.\n",
    "\n",
    "### Failure cases I saw (and how I tuned)\n",
    "\n",
    "Below are *example* failure modes you’ll see in enterprise RAG. These are the ones that matter in interviews.\n",
    "\n",
    "1) **Numeric / threshold queries** (“under 6 hours”, “cap at 40 hours”)  \n",
    "   - Symptom: dense retrieval returns “travel” but misses the exact threshold sentence.  \n",
    "   - Fix: increase BM25 weight (e.g., 0.25 → 0.35) + keep overlap so the number stays in same chunk.\n",
    "\n",
    "2) **Acronyms / codes** (“PTO carryover cap”, “VPN”, “S3”, “SSO”)  \n",
    "   - Symptom: dense retrieval over-generalizes to “time off policy” without the clause.  \n",
    "   - Fix: BM25 candidate pool larger + rerank top_k slightly higher.\n",
    "\n",
    "3) **Exceptions + negations** (“alcohol is *not* reimbursable”, “unless VP approval”)  \n",
    "   - Symptom: retrieves the general rule but drops the exception sentence.  \n",
    "   - Fix: slightly larger chunk size + overlap; rerank more candidates.\n",
    "\n",
    "4) **Multi-hop questions** (“Who approves and what’s the SLA?”)  \n",
    "   - Symptom: answer is split across two docs or two far-apart sections.  \n",
    "   - Fix: return more hits + do a second-pass “context merge” step (next).\n",
    "\n",
    "### 5–10 concrete failure examples (use these in your README)\n",
    "\n",
    "You can paste these into your GitHub README as “debug notes”:\n",
    "\n",
    "1. **Query:** “What happens if I submit receipts after 30 days?”  \n",
    "   **Fail:** retrieved reimbursement policy but missed “VP approval” sentence.  \n",
    "   **Change:** overlap ↑, rerank_k ↑.\n",
    "\n",
    "2. **Query:** “Can I book premium economy for a 7 hour flight?”  \n",
    "   **Fail:** retrieved travel doc but missed “manager approval” constraint.  \n",
    "   **Change:** chunk_size ↑ slightly so rule + constraint stay together.\n",
    "\n",
    "3. **Query:** “How much PTO can I carry over?”  \n",
    "   **Fail:** got PTO overview but missed the exact “40 hours” line.  \n",
    "   **Change:** BM25 weight ↑ to boost numeric line.\n",
    "\n",
    "4. **Query:** “Is alcohol reimbursable at team dinner?”  \n",
    "   **Fail:** answered “meals reimbursed” but dropped the “no alcohol” exception.  \n",
    "   **Change:** rerank_k ↑, chunk overlap ↑.\n",
    "\n",
    "5. **Query:** “When are unused PTO hours forfeited?”  \n",
    "   **Fail:** retrieved PTO policy but missed “January 31” sentence due to chunk split.  \n",
    "   **Change:** overlap ↑.\n",
    "\n",
    "6. **Query:** “What’s the approval SLA for expenses?”  \n",
    "   **Fail:** got policy but returned wrong SLA because another doc mentioned a different SLA.  \n",
    "   **Change:** reranker top_k ↑ so cross-encoder can disambiguate.\n",
    "\n",
    "7. **Query:** “Do I need to use corporate card for flights?”  \n",
    "   **Fail:** retrieved travel policy but returned generic procurement guidance.  \n",
    "   **Change:** dense_k ↑ (more candidates) + rerank.\n",
    "\n",
    "8. **Query:** “Can I take parental leave 18 months after adoption?”  \n",
    "   **Fail:** retrieved parental leave doc but missed “within 12 months” constraint.  \n",
    "   **Change:** chunk_size ↑ a bit, rerank_k ↑.\n",
    "\n",
    "9. **Query:** “Is economy mandatory for 5.5 hour flight?”  \n",
    "   **Fail:** retrieved travel doc but confused threshold boundary.  \n",
    "   **Change:** BM25 weight ↑; add eval examples around boundaries.\n",
    "\n",
    "10. **Query:** “Where do I submit PTO requests?”  \n",
    "    **Fail:** retrieved HR policy but missed the system name.  \n",
    "    **Change:** add more ‘system-name’ sentences to corpus; in real life this is a data-quality issue.\n",
    "\n",
    "The point isn’t that RAG “never fails” — it’s that you can **observe failures, explain them, and tune knobs deliberately**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
